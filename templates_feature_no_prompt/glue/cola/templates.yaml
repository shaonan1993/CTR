dataset: glue
subset: cola
templates:
  1d3f5f15-8128-4445-8de5-92365b7e54a8: !Template
    answer_choices: no ||| yes
    id: 1d3f5f15-8128-4445-8de5-92365b7e54a8
    jinja: '{{"yes"}} or {{"no"}}.

      {{sentence}}

      |||

      {{ answer_choices[label] }}'
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: Make sense yes no
    reference: ''
  39a701ff-bb4b-48ac-8c0a-8c61bf0d4b8d: !Template
    answer_choices: No ||| Yes
    id: 39a701ff-bb4b-48ac-8c0a-8c61bf0d4b8d
    jinja: '{{sentence}}

      |||

      {{ answer_choices[label] }}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: is_this_correct
    reference: A sample glue template
  6f49b860-9145-4fcb-b632-9faea39e254e: !Template
    answer_choices: no ||| yes
    id: 6f49b860-9145-4fcb-b632-9faea39e254e
    jinja: '
      {{sentence}}

      {{"yes or no"}}.

      |||

      {{ answer_choices[label] }}'
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: editing
    reference: ''
  79b4c04c-c0e2-4add-a600-d5572da192e7: !Template
    answer_choices: unacceptable ||| acceptable
    id: 79b4c04c-c0e2-4add-a600-d5572da192e7
    jinja: '{{"acceptable"}}  {{"unacceptable"}}
      {{sentence}}

      |||

      {{ answer_choices[label] }}'
    metadata: !TemplateMetadata
      choices_in_prompt: true
      metrics:
      - Accuracy
      original_task: true
    name: Following sentence acceptable
    reference: ''
  dd33f089-57a1-452b-8bd5-8f1fffd10b60: !Template
    answer_choices: no ||| yes
    id: dd33f089-57a1-452b-8bd5-8f1fffd10b60
    jinja: '{{sentence}}

      |||

      {{ answer_choices[label] }}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      metrics:
      - Accuracy
      original_task: true
    name: Previous sentence acceptable
    reference: ''
